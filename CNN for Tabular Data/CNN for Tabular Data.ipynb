{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Tabular Dataset\n",
    "### 2- Leaf classification dataset for multiclassifcation\n",
    "### 3- Dataset is too small for Deep learning Models\n",
    "### 4- This code is just to give an example how to use CNN on Tabular dataset for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load train and test data\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate train and test Dataframe\n",
    "df = pd.concat([df_train,df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "species      594\n",
       "margin1        0\n",
       "margin2        0\n",
       "margin3        0\n",
       "            ... \n",
       "texture60      0\n",
       "texture61      0\n",
       "texture62      0\n",
       "texture63      0\n",
       "texture64      0\n",
       "Length: 194, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check if their is any null value\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop index column\n",
    "df.drop([\"id\"], axis =1 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove Null \n",
    "df =df[~df.isin([np.nan, np.inf, -np.inf]).any(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 990 entries, 0 to 989\n",
      "Columns: 193 entries, species to texture64\n",
      "dtypes: float64(192), object(1)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get Output column\n",
    "Y = df[\"species\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get remaning columns\n",
    "X = df.drop([\"species\"], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding of Label for multiclassification\n",
    "Y = pd.get_dummies(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split test and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30, random_state=1 , stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split test and validation\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.50, random_state=2 , stratify= y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataframe to numpy\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "X_val= X_val.to_numpy()\n",
    "y_val = y_val.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "X_val = scaler.fit_transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X data to pass in Convolutional1D layers\n",
    "X_train = np.reshape(X_train, X_train.shape + (1,))\n",
    "X_test =  np.reshape(X_test, X_test.shape + (1,))\n",
    "X_val =  np.reshape(X_val, X_val.shape + (1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(693, 192, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for CNN model\n",
    "batchSize =  X_train.shape[0]\n",
    "length =  X_train.shape[1]\n",
    "channel = X_train.shape[2]\n",
    "n_outputs = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN Model\n",
    "def getModel():\n",
    "    #Initialising the CNN\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Convolution1D(filters= 8, kernel_initializer='he_uniform',  kernel_size=3, activation='relu',input_shape=(length, channel)))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Convolution1D(filters= 16,  kernel_initializer='he_uniform', kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2,strides=2))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Convolution1D(filters=32, kernel_size=5,  kernel_initializer='he_uniform', activation=\"relu\",input_shape=(length, channel)))\n",
    "    model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "\n",
    "    #2.Flattening\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #3.Full Connection\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer= \"adam\",  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\cct\\.conda\\envs\\pythonCPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 190, 8)            32        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 95, 8)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 95, 8)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 95, 8)             32        \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 93, 16)            400       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 46, 16)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 46, 16)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 46, 16)            64        \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 42, 32)            2592      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 20, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20, 32)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 20, 32)            128       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 640)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 640)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                41024     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 99)                6435      \n",
      "=================================================================\n",
      "Total params: 50,707\n",
      "Trainable params: 50,595\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = getModel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\cct\\.conda\\envs\\pythonCPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 693 samples, validate on 149 samples\n",
      "Epoch 1/30\n",
      "693/693 [==============================] - 1s 2ms/step - loss: 4.5832 - accuracy: 0.0476 - val_loss: 4.1035 - val_accuracy: 0.0671\n",
      "Epoch 2/30\n",
      "693/693 [==============================] - 0s 594us/step - loss: 3.6690 - accuracy: 0.1400 - val_loss: 3.0383 - val_accuracy: 0.3490\n",
      "Epoch 3/30\n",
      "693/693 [==============================] - 0s 582us/step - loss: 2.7419 - accuracy: 0.3175 - val_loss: 1.9912 - val_accuracy: 0.5772\n",
      "Epoch 4/30\n",
      "693/693 [==============================] - 0s 588us/step - loss: 1.8232 - accuracy: 0.5599 - val_loss: 1.2380 - val_accuracy: 0.7181\n",
      "Epoch 5/30\n",
      "693/693 [==============================] - 0s 600us/step - loss: 1.1903 - accuracy: 0.6984 - val_loss: 0.8764 - val_accuracy: 0.7785\n",
      "Epoch 6/30\n",
      "693/693 [==============================] - 0s 594us/step - loss: 0.8946 - accuracy: 0.7605 - val_loss: 0.6287 - val_accuracy: 0.8456\n",
      "Epoch 7/30\n",
      "693/693 [==============================] - 0s 603us/step - loss: 0.6810 - accuracy: 0.8153 - val_loss: 0.4962 - val_accuracy: 0.8792\n",
      "Epoch 8/30\n",
      "693/693 [==============================] - 0s 593us/step - loss: 0.4676 - accuracy: 0.8716 - val_loss: 0.4152 - val_accuracy: 0.8993\n",
      "Epoch 9/30\n",
      "693/693 [==============================] - 0s 601us/step - loss: 0.3499 - accuracy: 0.9134 - val_loss: 0.3753 - val_accuracy: 0.8993\n",
      "Epoch 10/30\n",
      "693/693 [==============================] - 0s 599us/step - loss: 0.3283 - accuracy: 0.9048 - val_loss: 0.3402 - val_accuracy: 0.9128\n",
      "Epoch 11/30\n",
      "693/693 [==============================] - 0s 604us/step - loss: 0.2252 - accuracy: 0.9495 - val_loss: 0.3296 - val_accuracy: 0.9060\n",
      "Epoch 12/30\n",
      "693/693 [==============================] - 0s 606us/step - loss: 0.2198 - accuracy: 0.9408 - val_loss: 0.3085 - val_accuracy: 0.8993\n",
      "Epoch 13/30\n",
      "693/693 [==============================] - 0s 607us/step - loss: 0.1973 - accuracy: 0.9408 - val_loss: 0.3186 - val_accuracy: 0.8926\n",
      "Epoch 14/30\n",
      "693/693 [==============================] - 0s 607us/step - loss: 0.2028 - accuracy: 0.9466 - val_loss: 0.2708 - val_accuracy: 0.9262\n",
      "Epoch 15/30\n",
      "693/693 [==============================] - 0s 610us/step - loss: 0.1202 - accuracy: 0.9798 - val_loss: 0.2776 - val_accuracy: 0.9195\n",
      "Epoch 16/30\n",
      "693/693 [==============================] - 0s 611us/step - loss: 0.1122 - accuracy: 0.9798 - val_loss: 0.2488 - val_accuracy: 0.9329\n",
      "Epoch 17/30\n",
      "693/693 [==============================] - 0s 616us/step - loss: 0.1198 - accuracy: 0.9683 - val_loss: 0.2478 - val_accuracy: 0.9060\n",
      "Epoch 18/30\n",
      "693/693 [==============================] - 0s 615us/step - loss: 0.0950 - accuracy: 0.9784 - val_loss: 0.2713 - val_accuracy: 0.9060\n",
      "Epoch 19/30\n",
      "693/693 [==============================] - 0s 617us/step - loss: 0.0813 - accuracy: 0.9827 - val_loss: 0.2453 - val_accuracy: 0.8993\n",
      "Epoch 20/30\n",
      "693/693 [==============================] - 0s 620us/step - loss: 0.0858 - accuracy: 0.9870 - val_loss: 0.2040 - val_accuracy: 0.9329\n",
      "Epoch 21/30\n",
      "693/693 [==============================] - 0s 620us/step - loss: 0.0845 - accuracy: 0.9812 - val_loss: 0.2025 - val_accuracy: 0.9396\n",
      "Epoch 22/30\n",
      "693/693 [==============================] - 0s 626us/step - loss: 0.0951 - accuracy: 0.9726 - val_loss: 0.1752 - val_accuracy: 0.9597\n",
      "Epoch 23/30\n",
      "693/693 [==============================] - 0s 625us/step - loss: 0.0782 - accuracy: 0.9812 - val_loss: 0.1946 - val_accuracy: 0.9396\n",
      "Epoch 24/30\n",
      "693/693 [==============================] - 0s 664us/step - loss: 0.0677 - accuracy: 0.9812 - val_loss: 0.2527 - val_accuracy: 0.9329\n",
      "Epoch 25/30\n",
      "693/693 [==============================] - 0s 646us/step - loss: 0.0636 - accuracy: 0.9856 - val_loss: 0.2400 - val_accuracy: 0.9195\n",
      "Epoch 26/30\n",
      "693/693 [==============================] - 0s 633us/step - loss: 0.0804 - accuracy: 0.9769 - val_loss: 0.1722 - val_accuracy: 0.9463\n",
      "Epoch 27/30\n",
      "693/693 [==============================] - 0s 641us/step - loss: 0.0613 - accuracy: 0.9870 - val_loss: 0.1783 - val_accuracy: 0.9262\n",
      "Epoch 28/30\n",
      "693/693 [==============================] - 0s 645us/step - loss: 0.0595 - accuracy: 0.9841 - val_loss: 0.2124 - val_accuracy: 0.9329\n",
      "Epoch 29/30\n",
      "693/693 [==============================] - 0s 676us/step - loss: 0.0528 - accuracy: 0.9798 - val_loss: 0.2061 - val_accuracy: 0.9396\n",
      "Epoch 30/30\n",
      "693/693 [==============================] - 0s 636us/step - loss: 0.0569 - accuracy: 0.9812 - val_loss: 0.1555 - val_accuracy: 0.9597\n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    verbose=1,\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    "    epochs=30,\n",
    "    validation_data=(X_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model weights\n",
    "file_path = r\"model_weights.h5\"\n",
    "model.save_weights(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148/148 [==============================] - 0s 815us/step\n"
     ]
    }
   ],
   "source": [
    "# Retrve Model , load weights and evaluate test data\n",
    "model = getModel()\n",
    "model.load_weights(file_path)\n",
    "score  = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy  :0.9527027010917664\n",
      "loss  :0.11453679307187731\n"
     ]
    }
   ],
   "source": [
    "# Score and Loss\n",
    "model_parameters = model.metrics_names\n",
    "print(f\"{model_parameters[1]}  :{score[1]}\")\n",
    "print(f\"{model_parameters[0]}  :{score[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonCPU",
   "language": "python",
   "name": "pythoncpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
