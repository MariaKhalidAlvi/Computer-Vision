{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\cct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Download Book Data for text generation\n",
    "nltk.download(\"book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for book data downloaded\n",
    "cospus_dir = r\"C:\\Users\\cct\\AppData\\Roaming\\nltk_data\\corpora\\state_union\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Book file paths\n",
    "file_paths = []\n",
    "for root, dirs, files in os.walk(cospus_dir, topdown = False):\n",
    "    \n",
    "      for name in files:\n",
    "            file_paths.append(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join text of all book files\n",
    "text = []\n",
    "for file in file_names:\n",
    "    \n",
    "    with open(file, \"r\") as f:\n",
    "        try:\n",
    "            text.append(f.read().replace(\"\\n\", \"\").lower())\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "text = \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char indices and indices char for unique characters in text\n",
    "unique_char = sorted(list(set(text)))\n",
    "\n",
    "char_indices = dict((char,idx) for idx, char in enumerate(unique_char)) \n",
    "indices_char = dict((idx,char) for idx, char in enumerate(unique_char)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences: 516664\n"
     ]
    }
   ],
   "source": [
    "# Sequences of text for train data\n",
    "sequence_length = 50 \n",
    "stride_Steps = 4 \n",
    "\n",
    "sequences = []\n",
    "next_sequence = []\n",
    "\n",
    "for i in range(0, len(text) - sequence_length, stride_Steps):\n",
    "    sequences.append(text[i: i + sequence_length])\n",
    "    next_sequence.append(text[i + sequence_length]) \n",
    "\n",
    "print('sequences:', len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare tensors for train x and y\n",
    "x = np.zeros((len(sequences), sequence_length, len(unique_char)), dtype=np.bool) \n",
    "y = np.zeros((len(sequences), len(unique_char)), dtype=np.bool) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare train x and y data by filling the tensors\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for k, char in enumerate(sequence):\n",
    "        x[i, k, char_indices[char]] = 1 \n",
    "    y[i, char_indices[next_sequence[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature sampling for introduce randomness in text generation\n",
    "def temperature_sampling(preds, temp):\n",
    "    \n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temp \n",
    "    preds_exp = np.exp(preds)\n",
    "    \n",
    "    preds = preds_exp / np.sum(preds_exp) \n",
    "    \n",
    "    prob = np.random.multinomial(1, preds, size=1) \n",
    "    return np.argmax(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text after every epoch\n",
    "\n",
    "def end_epoch(epoch, _):\n",
    "    \n",
    "    print(f\"**********Generating text after Epoch {epoch} ***********\")\n",
    "    \n",
    "    noOfChar = 200 # no of characters to be generated\n",
    "    \n",
    "    srt_idx = random.randint(0, len(text) - sequence_length - 1)   # start index for test sentence\n",
    "\n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]:  # temoeratures for randomness\n",
    "        \n",
    "        generated_text = ''\n",
    "        sequence = text[srt_idx: srt_idx + sequence_length]\n",
    "        generated_text = generated_text + sequence\n",
    "        \n",
    "        print(f\"With Temperature: {temperature}\")\n",
    "        print(f\"For sentence: {sequence}\")\n",
    "\n",
    "        for i in range(0,noOfChar):\n",
    "            \n",
    "            # fill tensor for test sentence\n",
    "            x_test = np.zeros((1, sequence_length, len(unique_char)))      \n",
    "            for k, char in enumerate(sequence):\n",
    "                x_test[0, k, char_indices[char]] = 1 \n",
    "            \n",
    "            # Predict Probabilities for next sequence\n",
    "            \n",
    "            x_pred= model.predict(x_test, verbose=0)[0]\n",
    "            \n",
    "            # Generate next character with different temperature values\n",
    "            next_char_index = temperature_sampling(x_pred, temperature) \n",
    "            next_char = indices_char[next_char_index]\n",
    "            \n",
    "            #update sentence with new character\n",
    "            sequence = sequence[1:] + next_char\n",
    "            \n",
    "            #Append new charater to generated text\n",
    "            generated_text = generated_text + next_char\n",
    "        \n",
    "       \n",
    "        print(f\" Generated Text: {generated_text}\")\n",
    "        \n",
    "    model.save_weights('LSTM_TextGeneration_weights.hdf5', overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "\n",
    "# Size of vector in the hidden layer.\n",
    "units = 128 \n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(units, input_shape=(x.shape[1], x.shape[2])))\n",
    "model.add(Dense(y.shape[1], activation='softmax')) \n",
    "\n",
    "# compile model with optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer= \"adam\") \n",
    "\n",
    "callback_results = LambdaCallback(on_epoch_end=end_epoch)\n",
    "checkpointer = ModelCheckpoint(filepath='model_weights.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "516664/516664 [==============================] - 470s 909us/step - loss: 2.2764\n",
      "**********Generating text after Epoch 0 ***********\n",
      "With Temperature: 0.2\n",
      "For sentence: ill maintain a nuclear deterrent adequate to meet \n",
      " Generated Text: ill maintain a nuclear deterrent adequate to meet the for to shat and the conter and the congres and in the promice be the for the promest the reand the the conting and the sear and the for cont the the the the in the conting and the conting on the c\n",
      "With Temperature: 0.5\n",
      "For sentence: ill maintain a nuclear deterrent adequate to meet \n",
      " Generated Text: ill maintain a nuclear deterrent adequate to meet will of ching ner censting of that the congen the with wise be fer in the the the indes protation the atien. in mese and the mare and the the werment tha s we rontent ic not ding and in the compan tha\n",
      "With Temperature: 1.0\n",
      "For sentence: ill maintain a nuclear deterrent adequate to meet \n",
      " Generated Text: ill maintain a nuclear deterrent adequate to meet pontcy this in benment face baigay s week the ixt ther lyominbinion gaas fou resps; the ruch onorghag. is comrsens of grogeen a ininat our progqentawe is plsentution this is conmenduve calic, perady t\n",
      "With Temperature: 1.2\n",
      "For sentence: ill maintain a nuclear deterrent adequate to meet \n",
      " Generated Text: ill maintain a nuclear deterrent adequate to meet ander com a dove seetat mibinn, whkindy foreing hacniwed nestwis sgm.awlas munh of conerine s freenale unffraco seroerigeag cound if recungu hatry cave parve oon erbabloge hot mqiency rommescit'an are\n",
      "Epoch 2/15\n",
      "   128/516664 [..............................] - ETA: 8:11 - loss: 2.1422"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cct\\.conda\\envs\\PythonCPU\\lib\\site-packages\\keras\\callbacks\\callbacks.py:707: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "516664/516664 [==============================] - 466s 901us/step - loss: 1.9256\n",
      "**********Generating text after Epoch 1 ***********\n",
      "With Temperature: 0.2\n",
      "For sentence: a federal bureaucrat may want, but for what their \n",
      " Generated Text: a federal bureaucrat may want, but for what their decain in the streation and the prople and the prople on the prople in the for the serican and the contrien and the propesting of the for and the streation and the comple and the prople and the seare \n",
      "With Temperature: 0.5\n",
      "For sentence: a federal bureaucrat may want, but for what their \n",
      " Generated Text: a federal bureaucrat may want, but for what their actions, and progral of the whold and priving and the fillien in lower and the comlarned and the now and frecting and dorice. the will of the promment for male of the portent of the propes the sucted \n",
      "With Temperature: 1.0\n",
      "For sentence: a federal bureaucrat may want, but for what their \n",
      " Generated Text: a federal bureaucrat may want, but for what their grogludizage shollen the says and instent to the contiges regrayes to chive in progration repling  suclionaon werk oun 3.55 rosten all--is whle gokill congrres go fill and fater, groutsation corte bil\n",
      "With Temperature: 1.2\n",
      "For sentence: a federal bureaucrat may want, but for what their \n",
      " Generated Text: a federal bureaucrat may want, but for what their esfarg thath bereacict.or frec supte alp a dustricl.- aiests, is andich on firlly.ny and novares coply. mibkemy.i ginsiction on is the by wow, ase.s- griscfigiessenmmea's usireso of taxioves sonarue e\n",
      "Epoch 3/15\n",
      "516664/516664 [==============================] - 450s 871us/step - loss: 1.7612\n",
      "**********Generating text after Epoch 2 ***********\n",
      "With Temperature: 0.2\n",
      "For sentence:  we've done so far, let no one say that this natio\n",
      " Generated Text:  we've done so far, let no one say that this nation of the security and the security of the sten we have the conting the endicies and the reconger to and a mest me the program to recong the security of the progress of the congress and the security to\n",
      "With Temperature: 0.5\n",
      "For sentence:  we've done so far, let no one say that this natio\n",
      " Generated Text:  we've done so far, let no one say that this nation of continied for meated no the long the new restral year and we mist my as the enfunition ho and conter to enverter with the sear the mast we hell american state the grengits and internments and the\n",
      "With Temperature: 1.0\n",
      "For sentence:  we've done so far, let no one say that this natio\n",
      " Generated Text:  we've done so far, let no one say that this nations of werse alf resorgers juving, atcesplitit anos our must not be greends continging the ugies their people in to conorting the remoctions inwtel buidech. his bremploris in the firching milling cansi\n",
      "With Temperature: 1.2\n",
      "For sentence:  we've done so far, let no one say that this natio\n",
      " Generated Text:  we've done so far, let no one say that this nationas, my abvutionnon nfighry.  we clinition, io kederads.indents whep teaper.yet be tates heold a giewt nem lestering and it ans our justion be, a taxerfeds asdatuty, hear it the and and ngaly.a days h\n",
      "Epoch 4/15\n",
      "516664/516664 [==============================] - 449s 869us/step - loss: 1.6540\n",
      "**********Generating text after Epoch 3 ***********\n",
      "With Temperature: 0.2\n",
      "For sentence: y. i am proposing this evening a change in the alt\n",
      " Generated Text: y. i am proposing this evening a change in the alte and the possice and the security to the contrical programs of the contriction and the congress of the sount of the congress to the congress of the continue that the congress to the will contriture t\n",
      "With Temperature: 0.5\n",
      "For sentence: y. i am proposing this evening a change in the alt\n",
      " Generated Text: y. i am proposing this evening a change in the alt in the country to the workers the are the contron and working the contrit and will be a lay the lising now the manion of the programs the sear to expending the government of the fore of this american\n",
      "With Temperature: 1.0\n",
      "For sentence: y. i am proposing this evening a change in the alt\n",
      " Generated Text: y. i am proposing this evening a change in the altoring wiesk peaction of our purposemonctive offeces poleryh plonger the hospedity of socurity hay to sen af begon e9wagn and sod to economicos; thes that in this 11 year's can oun economl. wet be uevi\n",
      "With Temperature: 1.2\n",
      "For sentence: y. i am proposing this evening a change in the alt\n",
      " Generated Text: y. i am proposing this evening a change in the alts inse grait. verues af abmilais proppomicionalfindsave tung that recation in as. toough reabrecreastrey, it scrimy -- (ansided o-copp cauring on entined uppary, slarami we will then recaudic everys. \n",
      "Epoch 5/15\n",
      "516664/516664 [==============================] - 445s 861us/step - loss: 1.5785\n",
      "**********Generating text after Epoch 4 ***********\n",
      "With Temperature: 0.2\n",
      "For sentence: e of millions elsewhere, and the exclusion of japa\n",
      " Generated Text: e of millions elsewhere, and the exclusion of japan and the firen in the firm to the force the first with the strength of the program to the for the for the for the federal the for the for the first to the for the for the for the first the for the fo\n",
      "With Temperature: 0.5\n",
      "For sentence: e of millions elsewhere, and the exclusion of japa\n",
      " Generated Text: e of millions elsewhere, and the exclusion of japanch of the vien and state of the beding our proposed with the dor a befide on the repormmint for the exte dements to the passive the continus to the defenses of the people of american peace a new sear\n",
      "With Temperature: 1.0\n",
      "For sentence: e of millions elsewhere, and the exclusion of japa\n",
      " Generated Text: e of millions elsewhere, and the exclusion of japans, to our our our actaided it dessivity part to saching people government is ctany to a more reartmans of nmmunting offection of whine ust can ccyons and and as naciam the tearged to amerateage. shav\n",
      "With Temperature: 1.2\n",
      "For sentence: e of millions elsewhere, and the exclusion of japa\n",
      " Generated Text: e of millions elsewhere, and the exclusion of japalionghtrieggeteman we nuw just quiler mest contllains you arout emeny who fouth plage.come shout sthe go proved us housk, the sing, is a fow was undual owhicare halp; inta, throam, medinats. we pordib\n",
      "Epoch 6/15\n",
      "516664/516664 [==============================] - 443s 857us/step - loss: 1.5234\n",
      "**********Generating text after Epoch 5 ***********\n",
      "With Temperature: 0.2\n",
      "For sentence: ly demonstrated to the world that america will not\n",
      " Generated Text: ly demonstrated to the world that america will not of the program to the state of our come to the program to the congress to the congress to the endugh to the congress to the fact of the federal security and a congress in the congress of the program \n",
      "With Temperature: 0.5\n",
      "For sentence: ly demonstrated to the world that america will not\n",
      " Generated Text: ly demonstrated to the world that america will not prosperity to the congress wart who suncer that the finent of the unitation problems and it is the increans and with the comprest the bend the war grade the entire of the program consting of the the \n",
      "With Temperature: 1.0\n",
      "For sentence: ly demonstrated to the world that america will not\n",
      " Generated Text: ly demonstrated to the world that america will noting the congres, with must read-lanber teren 3 yither can jouth and state spend a democraccis sys elsomed dations of to fole. i wame ratuins allay.their revely be toray. (applause, untere amery worker\n",
      "With Temperature: 1.2\n",
      "For sentence: ly demonstrated to the world that america will not\n",
      " Generated Text: ly demonstrated to the world that america will not mome rele nation acreaply dan rely shourtetion. legislation deyemment mostress, that have gins wost equal they decention ond a naw coulsment, sulation dolld. sgient--axheaded is more melionaly sert r\n",
      "Epoch 7/15\n",
      "516664/516664 [==============================] - 443s 858us/step - loss: 1.4816\n",
      "**********Generating text after Epoch 6 ***********\n",
      "With Temperature: 0.2\n",
      "For sentence: lion, 75th rangers. it's dated december 18th, the \n",
      " Generated Text: lion, 75th rangers. it's dated december 18th, the security and the strengther the many the military states and and the freedom and the next american progress the people the congress the security and the security and the security to the propose of the\n",
      "With Temperature: 0.5\n",
      "For sentence: lion, 75th rangers. it's dated december 18th, the \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Generated Text: lion, 75th rangers. it's dated december 18th, the semple. me are been lational decent of first the democration and wemple as expechation to the country that expenditures the world our are the destront and every strengthen the have so are expenditures\n",
      "With Temperature: 1.0\n",
      "For sentence: lion, 75th rangers. it's dated december 18th, the \n",
      " Generated Text: lion, 75th rangers. it's dated december 18th, the farminal regusares.reedivess jubully on sall, and cloperipublets -- we lawn education produching. the keep one abortantage 2500 percent to shand not even where suppriares. defense mmeniop being movelk\n",
      "With Temperature: 1.2\n",
      "For sentence: lion, 75th rangers. it's dated december 18th, the \n",
      " Generated Text: lion, 75th rangers. it's dated december 18th, the world pramestment or trus-sy't eaco at proflewbal that would our lexpons, and tixheh to hadd on enduralvatures ane an recent 6) the und dute essideration brater qutare yearsh-with sedlems, leg.think f\n",
      "Epoch 8/15\n",
      "516664/516664 [==============================] - 442s 855us/step - loss: 1.4478\n",
      "**********Generating text after Epoch 7 ***********\n",
      "With Temperature: 0.2\n",
      "For sentence: ars of work to do to do that.i don't want to destr\n",
      " Generated Text: ars of work to do to do that.i don't want to destrated the fact of the decentines of the farming the first to the congress that the proposed to the proposed to the united to propes to the production of the first the first to the proposed to the first\n",
      "With Temperature: 0.5\n",
      "For sentence: ars of work to do to do that.i don't want to destr\n",
      " Generated Text: ars of work to do to do that.i don't want to destrous of justion of the first control the disears of american in the proposing the nation congress in the war to expand of basic of the urate of the congress and health can be the congress with the cong\n",
      "With Temperature: 1.0\n",
      "For sentence: ars of work to do to do that.i don't want to destr\n",
      " Generated Text: ars of work to do to do that.i don't want to destron.  increciences with our americans that will needital, an gromparals these which a laterate and all frands. conicibul our ener should prominal sall our waile to readernation prosevinaly memand our i\n",
      "With Temperature: 1.2\n",
      "For sentence: ars of work to do to do that.i don't want to destr\n",
      " Generated Text: ars of work to do to do that.i don't want to destram, lar, improve to esneprlationarrald towage detement? um they shumbaryly:endoul years aflest fause rajoritution, detomed andelved, klowhe this miendopory. so was would freedom'bly strend nowle lemal\n",
      "Epoch 9/15\n",
      "516664/516664 [==============================] - 442s 856us/step - loss: 1.4211\n",
      "**********Generating text after Epoch 8 ***********\n",
      "With Temperature: 0.2\n",
      "For sentence: nate upon racial grounds and are unconstitutional.\n",
      " Generated Text: nate upon racial grounds and are unconstitutional. the security and support the seart in the united states of the tax recommended to american people is the progress the country of the program to the programs of the state of the program to continue to\n",
      "With Temperature: 0.5\n",
      "For sentence: nate upon racial grounds and are unconstitutional.\n",
      " Generated Text: nate upon racial grounds and are unconstitutional. we must new in the equal to the united state in the made to the most a military dollars and support to the national sall of the factt the long the federal for the tax been respect the sessome the cha\n",
      "With Temperature: 1.0\n",
      "For sentence: nate upon racial grounds and are unconstitutional.\n",
      " Generated Text: nate upon racial grounds and are unconstitutional. to keet the exergiving should health we some to the victory. we will keep udenwines biving one usion inous of the inceltic from daties tax sovied poblence, tonal you to ha've with us their made our c\n",
      "With Temperature: 1.2\n",
      "For sentence: nate upon racial grounds and are unconstitutional.\n",
      " Generated Text: nate upon racial grounds and are unconstitutional.in the terwy.our country modord, wienothers, withoug their than 35 us a ficlate.the, we should mostary dead nows. i have yought.nexes these wnolueberemeft cave strignam.i hel ale a matiome a nuchingsh\n",
      "Epoch 10/15\n",
      "516664/516664 [==============================] - 446s 863us/step - loss: 1.3982\n",
      "**********Generating text after Epoch 9 ***********\n",
      "With Temperature: 0.2\n",
      "For sentence: lanced federal budget.this is a joint responsibili\n",
      " Generated Text: lanced federal budget.this is a joint responsibility that we will not and the free men and the strength and the state of the program to a marte and the first the first the program to the program to the program to the congress and the constitution of \n",
      "With Temperature: 0.5\n",
      "For sentence: lanced federal budget.this is a joint responsibili\n",
      " Generated Text: lanced federal budget.this is a joint responsibilities that let us loce the have and defense of the fich of the signation of their country the american programs and but their committer the programs and formen people of the responsibility in the year,\n",
      "With Temperature: 1.0\n",
      "For sentence: lanced federal budget.this is a joint responsibili\n",
      " Generated Text: lanced federal budget.this is a joint responsibility and values of that is a lobleds to promit reserve \"as but our yet we naking americans; we form us dores. shend who heducathas and bettor has in the need scuppicarries policy has their by in the uni\n",
      "With Temperature: 1.2\n",
      "For sentence: lanced federal budget.this is a joint responsibili\n",
      " Generated Text: lanced federal budget.this is a joint responsibilities of hest partnershist-10 peace of pevervitiagialried -to 71-havel begle is dausing surpeustide.mant 50 cert of everyecing. shill from our chilations and jurted to dy that in secentinatols of buibl\n",
      "Epoch 11/15\n",
      "516664/516664 [==============================] - 443s 857us/step - loss: 1.3795\n",
      "**********Generating text after Epoch 10 ***********\n",
      "With Temperature: 0.2\n",
      "For sentence: led the child credit to $1,000 per child.  (applau\n",
      " Generated Text: led the child credit to $1,000 per child.  (applause.) and in the farm and the farm and the first and the farm of the progress the people and the failly to the first to the state the continue the last year and the first the common common commitment o\n",
      "With Temperature: 0.5\n",
      "For sentence: led the child credit to $1,000 per child.  (applau\n",
      " Generated Text: led the child credit to $1,000 per child.  (applause of the finance of the congress that in the congress to americans, we must deean the being our progress with the government of the interelt and marrer the great and community and determine of a land\n",
      "With Temperature: 1.0\n",
      "For sentence: led the child credit to $1,000 per child.  (applau\n",
      " Generated Text: led the child credit to $1,000 per child.  (applause, in the deess to jank in 1952 will, and hell with the future exedate engogentives and day their isequation war common americans have pay the weriate if or hold is mores we welf act raval rigkion ai\n",
      "With Temperature: 1.2\n",
      "For sentence: led the child credit to $1,000 per child.  (applau\n",
      " Generated Text: led the child credit to $1,000 per child.  (applause) weading of tidumations. undard cummantept new.undselmek of any chamberiate mit're'ser usemplionce of leaders mr.ilzent tothal wellaropores united business read:ow, lended to belus union. we hunkwi\n",
      "Epoch 12/15\n",
      "516664/516664 [==============================] - 439s 850us/step - loss: 1.3627\n",
      "**********Generating text after Epoch 11 ***********\n",
      "With Temperature: 0.2\n",
      "For sentence: and the resolution to take that course, then we sh\n",
      " Generated Text: and the resolution to take that course, then we should be a propose of the production and the farmers of the production and the state and the congress to the state of the state of the many of the herd the congress to a manifary support of the product\n",
      "With Temperature: 0.5\n",
      "For sentence: and the resolution to take that course, then we sh\n",
      " Generated Text: and the resolution to take that course, then we should will be interents and the responsibility of the congress of the stabte and the congress, our farmers of the land propes. the action to the line and in the best new crime to the say to increase, s\n",
      "With Temperature: 1.0\n",
      "For sentence: and the resolution to take that course, then we sh\n",
      " Generated Text: and the resolution to take that course, then we should warm agricish. tradinest upsts overces mecounes drug inliggom an accurded a mirating that protect of terin. such citizen singationmentional tations only specied some dizatecrecsupt hows to recomm\n",
      "With Temperature: 1.2\n",
      "For sentence: and the resolution to take that course, then we sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Generated Text: and the resolution to take that course, then we shalus in yon, and hoursely tallapted bedicies to moway while it cansit caple's sences, and our cord-apmrisal sentementable, that do, redicral engyers moro oursure gid bus.all broke matkara, rovel abwol\n",
      "Epoch 13/15\n",
      "516664/516664 [==============================] - 437s 847us/step - loss: 1.3482\n",
      "**********Generating text after Epoch 12 ***********\n",
      "With Temperature: 0.2\n",
      "For sentence:  ties of strategic interdependence. both nations n\n",
      " Generated Text:  ties of strategic interdependence. both nations no leadership of the strong the strong the possible the strong and the state of the future in the strong and the people in the world in the congress to the people and the state of the program and the p\n",
      "With Temperature: 0.5\n",
      "For sentence:  ties of strategic interdependence. both nations n\n",
      " Generated Text:  ties of strategic interdependence. both nations no the bourned to them our lising the state of all the people and health caboring the democracy and a markets of peace at the state of the blotery will be modern the union.  the procest to the surve th\n",
      "With Temperature: 1.0\n",
      "For sentence:  ties of strategic interdependence. both nations n\n",
      " Generated Text:  ties of strategic interdependence. both nations no shep, the nexter our long chamber with federal history rate working edulation of country.i've conidicape. (applause.).the without vote. tonight of our by all we must being more ary con this. i wave \n",
      "With Temperature: 1.2\n",
      "For sentence:  ties of strategic interdependence. both nations n\n",
      " Generated Text:  ties of strategic interdependence. both nations noo hoods was beaweier than hee foundly visitem. fod ulymential every 1982. $25 billion of troducally,?and amorg to achien will be fuer our between sthen is alrow, im roved made we sust we a has before\n",
      "Epoch 14/15\n",
      "516664/516664 [==============================] - 512s 991us/step - loss: 1.3355\n",
      "**********Generating text after Epoch 13 ***********\n",
      "With Temperature: 0.2\n",
      "For sentence: evel. as i have said, war and navy department requ\n",
      " Generated Text: evel. as i have said, war and navy department requires to the congress that we have a many of the first and the states and the soviet progress and our action to the since the production of the past of the soviet program of the structure of the securi\n",
      "With Temperature: 0.5\n",
      "For sentence: evel. as i have said, war and navy department requ\n",
      " Generated Text: evel. as i have said, war and navy department required in the world workers of state of the new protected by the support the support that we will months of the world. and the requires in the world in the history reached to fight to make the congress \n",
      "With Temperature: 1.0\n",
      "For sentence: evel. as i have said, war and navy department requ\n",
      " Generated Text: evel. as i have said, war and navy department requiring about.a fiscal redoct citizens, and addities and of that has next restoriti. aptring the we will soviet ement of serve westing of peace,, and we have houds on in our propose out than points to t\n",
      "With Temperature: 1.2\n",
      "For sentence: evel. as i have said, war and navy department requ\n",
      " Generated Text: evel. as i have said, war and navy department requirements those is rightide ests and improves we lonnoto ald-clear, low alo-aldifment knor, also 53 million any, and i thonever.  (applause.)these were plan. our owistan stoumaged at hame semildicuses,\n",
      "Epoch 15/15\n",
      "516664/516664 [==============================] - 1360s 3ms/step - loss: 1.3240\n",
      "**********Generating text after Epoch 14 ***********\n",
      "With Temperature: 0.2\n",
      "For sentence: here are many other desirable and needed tax chang\n",
      " Generated Text: here are many other desirable and needed tax changes to the congress and the world in the state of the security. the congress to the control of the need to the president the congress to provide the state of the first the world and the state of the pr\n",
      "With Temperature: 0.5\n",
      "For sentence: here are many other desirable and needed tax chang\n",
      " Generated Text: here are many other desirable and needed tax change of the private spending contral as new resources of the federal government is the american people of the constitution of the security who so make the community and some and fast the deal as a common\n",
      "With Temperature: 1.0\n",
      "For sentence: here are many other desirable and needed tax chang\n",
      " Generated Text: here are many other desirable and needed tax change have a seash of the matter] and we made a racks of deficities to fack over $270. spicks where south advers flege the hould teacher today and could keep our nations with the concervers trade together\n",
      "With Temperature: 1.2\n",
      "For sentence: here are many other desirable and needed tax chang\n",
      " Generated Text: here are many other desirable and needed tax changel in hand withoo yeach consumer-come.h this is i bearbane. averencess. 147, and its save their challenging dureph.ever progress that them with the divateal production if ecrop wement. and it betwaic \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x22b5e57d7b8>"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=15,\n",
    "          callbacks=[callback_results, checkpointer])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
